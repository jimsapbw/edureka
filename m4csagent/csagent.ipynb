{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af29402a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2081886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 Import necessary libraries\n",
    "\n",
    "import streamlit as st\n",
    "import asyncio\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import TypedDict, Optional, Dict, List\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f132022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 Load environment variables and configure logging\n",
    "\n",
    "# === CONFIG ===\n",
    "load_dotenv()\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bec32252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Define constants\n",
    "# INTENT_DETECTION_NODE = \"Intent Detection\": Defines a constant string that will be used as the name of the intent detection node in the LangGraph. This improves readability and makes it easier to refer to this node.\n",
    "INTENT_DETECTION_NODE = \"Intent Detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 State definition.\n",
    "# class AgentState(TypedDict): Defines a typed dictionary called AgentState.\n",
    "# This class specifies the structure of the agent's memory and metadata\n",
    "# that will be passed between nodes in the LangGraph.\n",
    "\n",
    "\n",
    "class AgentState(TypedDict): # Define the structure of the agent's state\n",
    "    # Metadata\n",
    "    user_id: str  # Unique identifier for the user\n",
    "    thread_id: str  # Identifier for the current conversation/thread\n",
    "    \n",
    "    # Core input/output\n",
    "    user_input: str  # Latest input from the user\n",
    "    intent: Optional[str]  # Detected intent of the user's input (e.g., \"query\", \"action\")\n",
    "    data: Optional[dict]  # Dictionary to store the output/response from a specific node\n",
    "    \n",
    "    # Memory\n",
    "    short_term_memory: Optional[List[Dict[str, str]]]  \n",
    "    # In-session memory: list of message dictionaries (e.g., {\"role\": \"user\", \"content\": \"...\"})\n",
    "    # Captures the immediate conversational context\n",
    "    \n",
    "    long_term_memory: Optional[Dict[str, List[Dict[str, str]]]]  \n",
    "    # Cross-session memory: dictionary keyed by categories (e.g., \"user_history\")\n",
    "    # Each category stores a list of past queries and resolutions\n",
    "    \n",
    "    # Flags\n",
    "    hitl_flag: Optional[bool]  \n",
    "    # Human-in-the-loop flag for high-risk queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7028cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5 === LLM ===This comment indicates the initialization of the language model.\n",
    "llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name=\"llama-3.3-70b-versatile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c2524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 Define MemorySaver for persistent memory storage\n",
    "\n",
    "class MemorySaver: # Simple in-memory storage for demonstration purposes\n",
    "    def __init__(self): # Initialize the memory store\n",
    "        self.store = {} # Dictionary to hold user memories\n",
    "\n",
    "    def load(self, user_id: str): # Load memory for a given user_id\n",
    "        return self.store.get(user_id) # Return the memory if it exists, else None\n",
    "\n",
    "    def save(self, user_id: str, memory: dict): # Save memory for a given user_id\n",
    "        self.store[user_id] = memory # Store the memory in the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1964294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 Initialize MemorySaver instance\n",
    "memory_saver = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d269779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 Define state reducer for message trimming and filtering\n",
    "\n",
    "def state_reducer(state: AgentState, new_message: Dict[str, str]) -> AgentState: # Define a reducer function to update state with new messages\n",
    "    \"\"\"\n",
    "    Reducer to update state with new messages while managing context window.\n",
    "    \n",
    "    Features:\n",
    "    - Filters out irrelevant messages (greetings, common pleasantries)\n",
    "    - Trims short-term memory to last 5 messages to manage context window size\n",
    "    \n",
    "    Args:\n",
    "        state: Current AgentState\n",
    "        new_message: Dictionary with \"role\" (user/assistant) and \"content\" keys\n",
    "    \n",
    "    Returns:\n",
    "        Updated AgentState with filtered and trimmed short-term memory\n",
    "    \"\"\"\n",
    "    # Get existing short-term memory\n",
    "    messages = state.get(\"short_term_memory\", [])\n",
    "    \n",
    "    # Filter out irrelevant messages (simple heuristic)\n",
    "    irrelevant_phrases = [\"hello\", \"hi\", \"thanks\", \"goodbye\", \"hey\", \"thank you\"]\n",
    "    if any(new_message[\"content\"].lower().strip().startswith(p) for p in irrelevant_phrases):\n",
    "        logger.info(f\"Filtered out irrelevant message: {new_message['content'][:20]}...\")\n",
    "        return state\n",
    "    \n",
    "    # Append new message\n",
    "    messages.append(new_message)\n",
    "    \n",
    "    # Trim to last 5 messages to manage context window\n",
    "    if len(messages) > 5:\n",
    "        messages = messages[-5:]\n",
    "        logger.info(\"Trimmed short-term memory to last 5 messages\")\n",
    "    \n",
    "    return {**state, \"short_term_memory\": messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3525a2",
   "metadata": {
    "vscode": {
     "languageId": "coffeescript"
    }
   },
   "outputs": [],
   "source": [
    "# Step 6 === USER HISTORY FETCH ===\n",
    "# This node retrieves past queries/resolutions from long-term memory\n",
    "# and enriches the current response with personalization.\n",
    "\n",
    "async def fetch_user_history(state: AgentState) -> AgentState: # Async function to fetch user history and personalize response\n",
    "    user_input = state['user_input'] # Get the latest user input\n",
    "    user_id = state.get('user_id') #    Get user ID from state\n",
    "    \n",
    "    # Load memory from MemorySaver\n",
    "    saved_memory = memory_saver.load(user_id) # Load saved memory for the user\n",
    "    if saved_memory: # If memory exists, retrieve long-term memory\n",
    "        long_term_memory = saved_memory.get('long_term_memory', {}) # Get long-term memory\n",
    "    else:\n",
    "        long_term_memory = state.get('long_term_memory', {}) # Fallback to state memory\n",
    "    \n",
    "    user_history = long_term_memory.get('user_history', []) # Get user history list\n",
    "    \n",
    "    try:\n",
    "        # Construct a prompt to personalize response\n",
    "        prompt = (\n",
    "            f\"User asked: {user_input}\\n\"\n",
    "            f\"Here is their past history: {user_history}\\n\"\n",
    "            f\"Provide a helpful response that references relevant past queries if applicable. \"\n",
    "            f\"Keep tone empathetic and clear.\"\n",
    "        ) # Create prompt for LLM\n",
    "        \n",
    "        # Use ainvoke with proper message format\n",
    "        response = await llm.ainvoke([{\"role\": \"user\", \"content\": prompt}]) # Call LLM asynchronously\n",
    "        message = response.content.strip() # Extract response message\n",
    "        \n",
    "        # Append current query to user history\n",
    "        user_history.append({\"query\": user_input, \"resolution\": message}) # Add new entry\n",
    "        \n",
    "        # Keep only last 5 entries to prevent unbounded growth\n",
    "        if len(user_history) > 5:\n",
    "            user_history = user_history[-5:] # Retain last 5 entries only\n",
    "        \n",
    "        long_term_memory[\"user_history\"] = user_history # Update long-term memory\n",
    "        \n",
    "        # Save updated memory to MemorySaver\n",
    "        memory_saver.save(user_id, {\"long_term_memory\": long_term_memory}) # Save memory\n",
    "        \n",
    "        logger.info(f\"Fetched user history for user_id: {user_id}\") # Log success\n",
    "        \n",
    "        return {\n",
    "            **state,\n",
    "            \"long_term_memory\": long_term_memory,\n",
    "            \"data\": {\"response\": message}\n",
    "        } # Return updated state with response\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in fetch_user_history: {e}\") # Log any errors\n",
    "        return {\n",
    "            **state,\n",
    "            \"data\": {\"response\": \"I apologize, but I encountered an error retrieving your history.\"} # Return error response\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84b568",
   "metadata": {
    "vscode": {
     "languageId": "coffeescript"
    }
   },
   "outputs": [],
   "source": [
    "# Step 7\n",
    "# === INTENT DETECTION FUNCTION ===\n",
    "# Defines an asynchronous function that takes the current AgentState\n",
    "# and returns an updated AgentState with the detected intent.\n",
    "\n",
    "async def detect_intent(state: AgentState) -> AgentState:\n",
    "    user_input = state['user_input']  # Extract latest user input\n",
    "    short_term_memory = state.get('short_term_memory', [])  # Should be List, not Dict\n",
    "    long_term_memory = state.get('long_term_memory', {})\n",
    "\n",
    "    # Construct prompt for LLM classification\n",
    "    prompt = (\n",
    "        \"Classify the user's intent into one of: \"\n",
    "        \"'faq', 'account_action', 'history', 'human_in_the_loop', or 'unknown'.\\n\"\n",
    "        f\"User input: {user_input}\\n\"\n",
    "        f\"Previous messages: {short_term_memory[-3:] if short_term_memory else 'none'}\\n\"\n",
    "        f\"Long-term context: {long_term_memory.get('user_history', 'none')}\\n\"\n",
    "        \"Respond with only the intent name.\"\n",
    "    )\n",
    "\n",
    "    # Call LLM with proper message format\n",
    "    response = await llm.ainvoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "    content = response.content.strip().lower()\n",
    "\n",
    "    # Match against predefined intents\n",
    "    match = re.search(r\"(faq|account_action|history|human_in_the_loop)\", content) # Regex to find intent\n",
    "    intent = match.group(1) if match else \"unknown\" # Default to \"unknown\" if no match\n",
    "\n",
    "    # Define high-risk keywords for HITL escalation\n",
    "    high_risk_keywords = [\n",
    "        \"error\", \"not working\", \"complaint\", \"refund\", \"charge dispute\",\n",
    "        \"escalate\", \"manager\", \"lawsuit\"\n",
    "    ] # Keywords indicating high-risk queries\n",
    "    hitl_flag = any(keyword in user_input.lower() for keyword in high_risk_keywords) # Check for high-risk keywords\n",
    "\n",
    "    logger.info(f\"Detected intent: {intent}, HITL flag: {hitl_flag}\")\n",
    "\n",
    "    # Use state_reducer to add user message to short-term memory\n",
    "    updated_state = state_reducer(state, {\"role\": \"user\", \"content\": user_input}) # Update state with new user message\n",
    "\n",
    "    return {\n",
    "        **updated_state,\n",
    "        \"intent\": intent,\n",
    "        \"hitl_flag\": hitl_flag\n",
    "    } # Return updated state with intent and HITL flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "313bad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11 Define FAQ Query Node\n",
    "\n",
    "async def faq_query_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    FAQ Query Node - Handles frequently asked questions using LLM.\n",
    "    \n",
    "    Responds to common queries like:\n",
    "    - Password reset\n",
    "    - Account setup\n",
    "    - Billing questions\n",
    "    - Feature information\n",
    "    \n",
    "    Args:\n",
    "        state: Current AgentState with user_input\n",
    "    \n",
    "    Returns:\n",
    "        Updated AgentState with FAQ response in data field\n",
    "    \"\"\"\n",
    "    user_input = state['user_input']\n",
    "    short_term_memory = state.get('short_term_memory', []) # Get short-term memory\n",
    "    \n",
    "    try:\n",
    "        # Define common FAQ knowledge base\n",
    "        faq_context = \"\"\"\n",
    "        Common FAQs:\n",
    "        - Password Reset: Go to Settings > Security > Reset Password. Click 'Forgot Password' and follow email instructions.\n",
    "        - Account Setup: Navigate to Profile > Complete Setup Wizard. Ensure all required fields are filled.\n",
    "        - Billing: View invoices under Billing > History. Update payment method in Billing > Payment Methods.\n",
    "        - Feature Access: Premium features require subscription upgrade. Go to Plans > Upgrade.\n",
    "        - Contact Support: Email support@company.com or use in-app chat for urgent issues.\n",
    "        \"\"\" # FAQ knowledge base\n",
    "        \n",
    "        # Construct prompt with FAQ context and conversation history\n",
    "        conversation_context = \"\\n\".join(\n",
    "            [f\"{msg['role']}: {msg['content']}\" for msg in short_term_memory[-3:]]\n",
    "        ) if short_term_memory else \"No previous context\" # Last 3 messages\n",
    "        \n",
    "        prompt = (\n",
    "            f\"{faq_context}\\n\\n\"\n",
    "            f\"Previous conversation:\\n{conversation_context}\\n\\n\"\n",
    "            f\"User question: {user_input}\\n\\n\"\n",
    "            f\"Provide a clear, helpful answer based on the FAQ knowledge above. \"\n",
    "            f\"If the question is not covered in FAQs, politely suggest contacting support. \"\n",
    "            f\"Keep response concise and actionable.\"\n",
    "        )\n",
    "        \n",
    "        # Call LLM to generate FAQ response\n",
    "        response = await llm.ainvoke([{\"role\": \"user\", \"content\": prompt}])\n",
    "        message = response.content.strip()\n",
    "        \n",
    "        logger.info(f\"FAQ Query processed: {user_input[:50]}...\")\n",
    "        \n",
    "        # Update state with assistant's response\n",
    "        updated_state = state_reducer(state, {\"role\": \"assistant\", \"content\": message})\n",
    "        \n",
    "        return {\n",
    "            **updated_state,\n",
    "            \"data\": {\"response\": message, \"source\": \"faq\"}\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in faq_query_node: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"data\": {\"response\": \"I apologize, but I encountered an error processing your FAQ query. Please try again or contact support.\"}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb02b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 Define Human-in-the-Loop Node\n",
    "\n",
    "async def human_in_the_loop(state: AgentState) -> AgentState: # Define HITL node for high-risk queries\n",
    "    \"\"\"\n",
    "    Human-in-the-Loop Node - Handles high-risk queries that require human review.\n",
    "    \n",
    "    Triggered when:\n",
    "    - hitl_flag is True (queries containing error, complaint, refund, etc.)\n",
    "    - Intent is explicitly 'human_in_the_loop'\n",
    "    \n",
    "    Args:\n",
    "        state: Current AgentState with user_input and hitl_flag\n",
    "    \n",
    "    Returns:\n",
    "        Updated AgentState with escalation message in data field\n",
    "    \"\"\"\n",
    "    user_input = state['user_input']\n",
    "    user_id = state.get('user_id', 'unknown')\n",
    "    thread_id = state.get('thread_id', 'unknown')\n",
    "    \n",
    "    try:\n",
    "        # Construct escalation message\n",
    "        prompt = (\n",
    "            f\"The query '{user_input}' has been flagged as high-risk and requires human review.\\n\\n\"\n",
    "            f\"A support specialist will review your case shortly. \"\n",
    "            f\"Your ticket ID is: {thread_id}\\n\\n\"\n",
    "            f\"In the meantime, you can:\\n\"\n",
    "            f\"- Email us at support@company.com with your ticket ID\\n\"\n",
    "            f\"- Check our status page for any ongoing issues\\n\"\n",
    "            f\"- Review our help documentation\\n\\n\"\n",
    "            f\"We appreciate your patience and will respond within 2 business hours.\"\n",
    "        )\n",
    "        \n",
    "        message = prompt\n",
    "        \n",
    "        # Log the escalation for human review\n",
    "        logger.warning(\n",
    "            f\"HITL Escalation - User: {user_id}, Thread: {thread_id}, \"\n",
    "            f\"Query: {user_input[:100]}...\"\n",
    "        ) # Log HITL escalation\n",
    "        \n",
    "        # Update state with assistant's escalation message\n",
    "        updated_state = state_reducer(state, {\"role\": \"assistant\", \"content\": message}) #   Update state with escalation message\n",
    "        \n",
    "        return {\n",
    "            **updated_state,\n",
    "            \"data\": {\n",
    "                \"response\": message,\n",
    "                \"escalated\": True,\n",
    "                \"ticket_id\": thread_id\n",
    "            }\n",
    "        } # Return updated state with escalation info\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in human_in_the_loop: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"data\": {\n",
    "                \"response\": \"Your query has been escalated to our support team. Please contact support@company.com for immediate assistance.\",\n",
    "                \"escalated\": True\n",
    "            }\n",
    "        } # Return updated state with escalation info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4bfb103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 13 Define Fallback Node\n",
    "\n",
    "async def fallback(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Fallback Node - Handles unknown or unclassified intents.\n",
    "    \n",
    "    Triggered when:\n",
    "    - Intent is 'unknown'\n",
    "    - No other node matches the user's request\n",
    "    \n",
    "    Provides helpful guidance on what the agent can assist with.\n",
    "    \n",
    "    Args:\n",
    "        state: Current AgentState with user_input\n",
    "    \n",
    "    Returns:\n",
    "        Updated AgentState with fallback message in data field\n",
    "    \"\"\"\n",
    "    user_input = state['user_input']\n",
    "    \n",
    "    try:\n",
    "        # Construct helpful fallback message\n",
    "        message = (\n",
    "            \"ü§î I'm not sure I understood your request.\\n\\n\"\n",
    "            \"I can help you with:\\n\"\n",
    "            \"‚Ä¢ **FAQs** - Password reset, account setup, billing questions\\n\"\n",
    "            \"‚Ä¢ **Account Actions** - Update profile, manage settings\\n\"\n",
    "            \"‚Ä¢ **History** - Review your past queries and interactions\\n\"\n",
    "            \"‚Ä¢ **Complex Issues** - Escalate to human support\\n\\n\"\n",
    "            \"Please rephrase your question or try asking about one of these topics.\"\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Fallback triggered for input: {user_input[:50]}...\")\n",
    "        \n",
    "        # Update state with assistant's fallback message\n",
    "        updated_state = state_reducer(state, {\"role\": \"assistant\", \"content\": message})\n",
    "        \n",
    "        return {\n",
    "            **updated_state,\n",
    "            \"data\": {\"response\": message, \"source\": \"fallback\"}\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in fallback: {e}\")\n",
    "        return {\n",
    "            **state,\n",
    "            \"data\": {\"response\": \"I apologize, but I encountered an error. Please try again or contact support.\"}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "80f31bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Customer support graph compiled successfully\n"
     ]
    }
   ],
   "source": [
    "# Step 14 Build & Compile Graph for Customer Support App\n",
    "\n",
    "# Define routing function to determine next node based on intent\n",
    "def route_to_node(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Determines the next node to execute based on the current state.\n",
    "    \n",
    "    Routing logic:\n",
    "    1. If hitl_flag is True -> escalate to human support\n",
    "    2. If intent is valid -> route to corresponding handler\n",
    "    3. Otherwise -> fallback node\n",
    "    \n",
    "    Args:\n",
    "        state: Current AgentState with intent and hitl_flag\n",
    "    \n",
    "    Returns:\n",
    "        Name of the next node to execute\n",
    "    \"\"\"\n",
    "    # Check for human escalation first\n",
    "    if state.get(\"hitl_flag\", False):\n",
    "        return \"human_in_the_loop\"\n",
    "    \n",
    "    # Define valid intents that map to specific nodes\n",
    "    valid_intents = [\"faq\", \"account_action\", \"history\"]\n",
    "    \n",
    "    # Route based on intent\n",
    "    intent = state.get(\"intent\", \"unknown\")\n",
    "    if intent in valid_intents:\n",
    "        return intent\n",
    "    \n",
    "    # Default to fallback for unknown intents\n",
    "    return \"fallback\"\n",
    "\n",
    "\n",
    "# Initialize the StateGraph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(INTENT_DETECTION_NODE, detect_intent)\n",
    "workflow.add_node(\"faq\", faq_query_node)\n",
    "workflow.add_node(\"history\", fetch_user_history)\n",
    "workflow.add_node(\"human_in_the_loop\", human_in_the_loop)\n",
    "workflow.add_node(\"fallback\", fallback)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(INTENT_DETECTION_NODE)\n",
    "\n",
    "# Add conditional edges from intent detection to appropriate handlers\n",
    "workflow.add_conditional_edges(\n",
    "    INTENT_DETECTION_NODE,\n",
    "    route_to_node,\n",
    "    {\n",
    "        \"faq\": \"faq\",\n",
    "        \"history\": \"history\",\n",
    "        \"human_in_the_loop\": \"human_in_the_loop\",\n",
    "        \"fallback\": \"fallback\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add finish edges (all handler nodes go to END)\n",
    "workflow.add_edge(\"faq\", \"__end__\")\n",
    "workflow.add_edge(\"history\", \"__end__\")\n",
    "workflow.add_edge(\"human_in_the_loop\", \"__end__\")\n",
    "workflow.add_edge(\"fallback\", \"__end__\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "logger.info(\"Customer support graph compiled successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8be2a5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 12:19:49.870 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.872 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.978 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\jimit\\edmods\\m4csagent\\env4a\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-12-17 12:19:49.979 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.982 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.984 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.984 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.985 Session state does not function when running a script without `streamlit run`\n",
      "2025-12-17 12:19:49.987 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.987 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.988 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.989 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.989 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.990 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.991 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.991 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.992 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.993 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.995 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.996 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.998 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:49.999 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-12-17 12:19:50.000 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Step 15 Build Streamlit UI for Customer Support App\n",
    "\n",
    "st.set_page_config(page_title=\"üéß Support Agent\", page_icon=\"üí¨\", layout=\"centered\")\n",
    "st.title(\"üéß AI Customer Support Agent\")\n",
    "st.caption(\"Get instant help with FAQs, account issues, and personalized support based on your history.\")\n",
    "\n",
    "# Initialize session state for messages, user_id, and thread_id\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "if \"user_id\" not in st.session_state:\n",
    "    st.session_state.user_id = \"user_001\"  # Default user ID (can be customized)\n",
    "if \"thread_id\" not in st.session_state:\n",
    "    st.session_state.thread_id = \"thread_001\"  # Default thread ID (can be customized)\n",
    "\n",
    "# Display chat messages from session state\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Handle user input\n",
    "if user_input := st.chat_input(\"Type your message...\"):\n",
    "    # Append user message to session state\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_input)\n",
    "    \n",
    "    # Display assistant's response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Processing your request...\"):\n",
    "            # Initialize state for the agent\n",
    "            state = {\n",
    "                \"user_id\": st.session_state.user_id,\n",
    "                \"thread_id\": st.session_state.thread_id,\n",
    "                \"user_input\": user_input,\n",
    "                \"intent\": None,\n",
    "                \"data\": None,\n",
    "                \"short_term_memory\": [],\n",
    "                \"long_term_memory\": {},\n",
    "                \"hitl_flag\": False\n",
    "            }\n",
    "            \n",
    "            # Invoke the compiled graph\n",
    "            final_state = asyncio.run(app.ainvoke(state))\n",
    "            bot_reply = final_state['data']['response']\n",
    "            \n",
    "            # Display the assistant's reply\n",
    "            st.markdown(bot_reply)\n",
    "    \n",
    "    # Append assistant message to session state\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": bot_reply})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1852e965",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 75) (2958854279.py, line 75)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m- Loads saved memory from `MemorySaver` for Sarah's user_id\u001b[39m\n                                                     ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 75)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# AI Customer Support App - User Journey Examples\n",
    "\"\"\"\n",
    "\n",
    "## üéØ Overview\n",
    "This AI-powered customer support agent uses LangGraph to intelligently route user queries through multiple specialized nodes. It features intent detection, memory management, and human-in-the-loop escalation for high-risk queries.\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Example 1: John - Password Reset (FAQ Query)\n",
    "\n",
    "### Step 1: User Input\n",
    "John types into the Streamlit chat:\n",
    "```\n",
    "How do I reset my password?\n",
    "```\n",
    "\n",
    "### Step 2: Entry Point -> Intent Detection\n",
    "- The graph starts at **INTENT_DETECTION_NODE**\n",
    "- The LLM classifies the intent as **\"faq\"**\n",
    "- No high-risk keywords detected -> `hitl_flag = False`\n",
    "- User message added to short-term memory\n",
    "\n",
    "### Step 3: Conditional Edge Routing\n",
    "The graph checks conditional edges:\n",
    "```python\n",
    "{\n",
    "  \"faq\": \"faq\",\n",
    "  \"history\": \"history\",\n",
    "  \"human_in_the_loop\": \"human_in_the_loop\",\n",
    "  \"fallback\": \"fallback\"\n",
    "}\n",
    "```\n",
    "- Since intent is **\"faq\"**, flow routes to the **FAQ Query Node**\n",
    "\n",
    "### Step 4: FAQ Query Node\n",
    "- `faq_query_node` retrieves FAQ knowledge base\n",
    "- LLM generates response using FAQ context\n",
    "- Updates state:\n",
    "```python\n",
    "{\n",
    "  \"data\": {\n",
    "    \"response\": \"Go to Settings > Security > Reset Password. Click 'Forgot Password' and follow email instructions.\",\n",
    "    \"source\": \"faq\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "- Assistant message added to short-term memory\n",
    "\n",
    "### Step 5: Output to User\n",
    "John sees:\n",
    "```\n",
    "Go to Settings > Security > Reset Password. Click 'Forgot Password' \n",
    "and follow email instructions.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Example 2: Sarah - History Query\n",
    "\n",
    "### Step 1: User Input\n",
    "Sarah types:\n",
    "```\n",
    "What did I ask you before?\n",
    "```\n",
    "\n",
    "### Step 2: Intent Detection\n",
    "- Intent classified as **\"history\"**\n",
    "- `hitl_flag = False`\n",
    "\n",
    "### Step 3: Routing\n",
    "- Flow goes to **History Handler** (`fetch_user_history`)\n",
    "\n",
    "### Step 4: User History Node\n",
    "- Loads saved memory from `MemorySaver` for Sarah's user_id\n",
    "- Retrieves past queries from long-term memory\n",
    "- LLM personalizes response based on user history\n",
    "- Updates long-term memory with current query\n",
    "- Saves back to `MemorySaver`\n",
    "\n",
    "### Step 5: Output\n",
    "Sarah sees:\n",
    "```\n",
    "Based on your history, you previously asked about password reset and \n",
    "billing questions. You were trying to update your payment method last time. \n",
    "Is there anything specific from your past queries you'd like me to help with?\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üö® Example 3: Mike - High-Risk Query (HITL Escalation)\n",
    "\n",
    "### Step 1: User Input\n",
    "Mike types:\n",
    "```\n",
    "This feature is not working and I want a refund immediately!\n",
    "```\n",
    "\n",
    "### Step 2: Intent Detection\n",
    "- Intent classified as **\"faq\"** or **\"account_action\"**\n",
    "- **High-risk keywords detected**: \"not working\", \"refund\"\n",
    "- `hitl_flag = True` üö©\n",
    "\n",
    "### Step 3: Routing\n",
    "- Despite intent being \"faq\", the routing function checks `hitl_flag` **first**\n",
    "```python\n",
    "if state.get(\"hitl_flag\", False):\n",
    "    return \"human_in_the_loop\"\n",
    "```\n",
    "- Flow routes to **Human-in-the-Loop Node**\n",
    "\n",
    "### Step 4: HITL Node\n",
    "- Constructs escalation message with ticket ID (thread_id)\n",
    "- Logs warning to terminal for human review:\n",
    "```\n",
    "HITL Escalation - User: user_001, Thread: thread_001, \n",
    "Query: This feature is not working and I want a refund...\n",
    "```\n",
    "- Updates state:\n",
    "```python\n",
    "{\n",
    "  \"data\": {\n",
    "    \"response\": \"A support specialist will review your case...\",\n",
    "    \"escalated\": True,\n",
    "    \"ticket_id\": \"thread_001\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 5: Output\n",
    "Mike sees:\n",
    "```\n",
    "The query 'This feature is not working and I want a refund immediately!' \n",
    "has been flagged as high-risk and requires human review.\n",
    "\n",
    "A support specialist will review your case shortly. Your ticket ID is: thread_001\n",
    "\n",
    "In the meantime, you can:\n",
    "- Email us at support@company.com with your ticket ID\n",
    "- Check our status page for any ongoing issues\n",
    "- Review our help documentation\n",
    "\n",
    "We appreciate your patience and will respond within 2 business hours.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ùì Example 4: Alex - Unknown Query (Fallback)\n",
    "\n",
    "### Step 1: User Input\n",
    "Alex types:\n",
    "```\n",
    "Tell me a joke\n",
    "```\n",
    "\n",
    "### Step 2: Intent Detection\n",
    "- LLM cannot classify into known intents\n",
    "- Intent marked as **\"unknown\"**\n",
    "- `hitl_flag = False`\n",
    "\n",
    "### Step 3: Routing\n",
    "- No match in valid intents [\"faq\", \"history\"]\n",
    "- Routes to **Fallback Node**\n",
    "\n",
    "### Step 4: Fallback Node\n",
    "- Generates helpful guidance message\n",
    "- Explains what the agent can assist with\n",
    "\n",
    "### Step 5: Output\n",
    "Alex sees:\n",
    "```\n",
    "ü§î I'm not sure I understood your request.\n",
    "\n",
    "I can help you with:\n",
    "‚Ä¢ FAQs - Password reset, account setup, billing questions\n",
    "‚Ä¢ Account Actions - Update profile, manage settings\n",
    "‚Ä¢ History - Review your past queries and interactions\n",
    "‚Ä¢ Complex Issues - Escalate to human support\n",
    "\n",
    "Please rephrase your question or try asking about one of these topics.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Summary\n",
    "\n",
    "| Query Type | Intent Detected | Route Taken | Key Features |\n",
    "|------------|----------------|-------------|--------------|\n",
    "| Password reset | `faq` | FAQ Node | Knowledge base + LLM |\n",
    "| Previous queries | `history` | History Node | Long-term memory retrieval |\n",
    "| Refund complaint | Any + HITL flag | Human-in-the-Loop | Keyword detection ‚Üí escalation |\n",
    "| Random question | `unknown` | Fallback | Helpful guidance |\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ State Flow Through Graph\n",
    "\n",
    "```\n",
    "User Input \n",
    "    ‚Üì\n",
    "Intent Detection Node\n",
    "    ‚îú‚îÄ Detect intent via LLM\n",
    "    ‚îú‚îÄ Check high-risk keywords\n",
    "    ‚îú‚îÄ Set hitl_flag\n",
    "    ‚îî‚îÄ Update short-term memory\n",
    "    ‚Üì\n",
    "Conditional Routing\n",
    "    ‚îú‚îÄ If hitl_flag=True -> Human-in-the-Loop ‚ö†Ô∏è\n",
    "    ‚îú‚îÄ If intent=\"faq\" or \"account_action\" -> FAQ Node üí¨\n",
    "    ‚îú‚îÄ If intent=\"history\" -> History Node üìö\n",
    "    ‚îî‚îÄ Else -> Fallback Node ‚ùì\n",
    "    ‚Üì\n",
    "Handler Node Processing\n",
    "    ‚îú‚îÄ Generate response\n",
    "    ‚îú‚îÄ Update memories\n",
    "    ‚îî‚îÄ Return state with data\n",
    "    ‚Üì\n",
    "END (Display to User)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Key Design Principles\n",
    "\n",
    "1. **HITL Priority**: High-risk queries bypass normal routing for immediate escalation\n",
    "2. **Memory Management**: Both short-term (5 messages) and long-term (5 history entries) are trimmed automatically\n",
    "3. **Empathetic Responses**: Clear, professional tone with actionable guidance\n",
    "4. **State Immutability**: Each node returns updated state without mutating input\n",
    "5. **Logging**: All intents, flags, and escalations are logged for debugging\n",
    "\n",
    "‚úÖ **This demonstrates how LangGraph's conditional edges + state management orchestrate intelligent customer support workflows seamlessly!**\n",
    "\"\"\"\n",
    "\n",
    "‚úÖ **This demonstrates how LangGraph's conditional edges + state management orchestrate intelligent customer support workflows seamlessly!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86679e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env4a (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
